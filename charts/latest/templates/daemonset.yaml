apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: {{ .Values.name }}-node
  labels:
    app: {{ .Values.name }}
    app.kubernetes.io/component: csi-local-node
  {{- include "chart.labels" . | nindent 4 }}
spec:
  selector:
    matchLabels:
      app: {{ .Values.name }}-node
      app.kubernetes.io/component: csi-local-node
    {{- include "chart.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        app: {{ .Values.name }}-node
        app.kubernetes.io/component: csi-local-node
      {{- include "chart.selectorLabels" . | nindent 8 }}
      annotations:
        kubectl.kubernetes.io/default-container: driver
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values:
                - amd64
                - arm64
              - key: kubernetes.io/os
                operator: In
                values:
                - linux
      {{- if .Values.raid.enabled }}
      initContainers:
      - name: raid
        {{- if hasPrefix "/" .Values.image.driver.repository }}
        image: "{{ .Values.image.baseRepo }}{{ .Values.image.driver.repository }}:{{ .Values.image.driver.tag | default .Chart.AppVersion }}"
        {{- else }}
        image: "{{ .Values.image.driver.repository }}:{{ .Values.image.driver.tag | default .Chart.AppVersion }}"
        {{- end }}
        imagePullPolicy: {{ .Values.image.driver.pullPolicy }}
        command:
        - nsenter
        - --target
        - "1"
        - --mount
        - --uts
        - --ipc
        - --net
        - --pid
        - --
        - bash
        - -exc
        - |
          set -x
          set -e
          set -o pipefail

          RAID_DEVICE="/dev/md0"
          VOLUME_GROUP="{{ .Values.raid.volumeGroup }}"

          ALL_NVME_DEVICES=$(ls /dev/nvme*n* 2>/dev/null)
          UNUSED_DEVICES=()

          if ! command -v mdadm &> /dev/null; then
                  echo "mdadm not found, installing..."
                  if command -v tdnf &> /dev/null; then
                          tdnf install -y mdadm
                  elif command -v apt-get &> /dev/null; then
                          apt-get update >/dev/null && apt-get install -y mdadm
                  else
                          echo "Error: Neither tdnf nor apt-get found. Cannot install mdadm."
                          exit 1
                  fi
          fi

          for device in $ALL_NVME_DEVICES; do
                  if mdadm --examine "$device" | grep -q 'RAID superblock'; then
                          continue
                  fi

                  if findmnt -S "$device" -n &> /dev/null; then
                          continue
                  fi
                  if blkid "$device" &> /dev/null; then
                          continue
                  fi
                  UNUSED_DEVICES+=("$device")
          done

          if [ "${#UNUSED_DEVICES[@]}" -eq 1 ]; then
                  echo "Only one unused NVMe device found: ${UNUSED_DEVICES[0]}"
                  DEVICE="${UNUSED_DEVICES[0]}"
                  echo "Creating LVM volume group ${VOLUME_GROUP} on ${DEVICE}"
                  pvcreate "${DEVICE}"
                  vgcreate "${VOLUME_GROUP}" "${DEVICE}"
                  echo "LVM volume group ${VOLUME_GROUP} created successfully."
                  exit 0
          fi

          if [ "${#UNUSED_DEVICES[@]}" -lt 2 ]; then
                  echo "Error: Found fewer than 2 unused NVMe devices.  Cannot create a RAID0 array."
                  exit 1
          fi

          if [ -e ${RAID_DEVICE} ]; then
                  echo "RAID device already exists"
          else
                  echo ""
                  echo "${#UNUSED_DEVICES[@]} devices will be combined into a RAID0 array:"
                  for device in "${UNUSED_DEVICES[@]}"; do
                          echo "  - $device"
                  done
                  echo ""

                  mdadm --create "$RAID_DEVICE" \
                          --level=0 \
                          --raid-devices="${#UNUSED_DEVICES[@]}" \
                          "${UNUSED_DEVICES[@]}" \
                          --run \
                          --force

                  if [ $? -ne 0 ]; then
                          echo "Error: Failed to create RAID array."
                          sleep 30
                          exit 1
                  fi

                  echo "RAID array created successfully."
                  echo "Waiting a few seconds for the device to be ready"
                  sleep 5

                  mkdir -p /etc/mdadm
                  mdadm --detail --scan | tee -a /etc/mdadm/mdadm.conf
          fi

          if [ ! -e ${RAID_DEVICE} ]; then
                  echo "Error: RAID device ${RAID_DEVICE} not found after creation."
                  exit 1
          fi

          if vgdisplay "${VOLUME_GROUP}" &> /dev/null; then
                  echo "Volume group ${VOLUME_GROUP} already exists."
                  exit 0
          fi

          echo "Creating LVM volume group ${VOLUME_GROUP} on ${RAID_DEVICE}"
          pvcreate "${RAID_DEVICE}"
          vgcreate "${VOLUME_GROUP}" "${RAID_DEVICE}"
          echo "LVM volume group ${VOLUME_GROUP} created successfully."
          exit 0

        securityContext:
          runAsUser: 0
          privileged: true
      {{- end }}

      containers:

      - name: driver
        args:
        - --node-name=$(NODE_NAME)
        - --pod-name=$(POD_NAME)
        - --namespace=$(POD_NAMESPACE)
        - --csi-bind-address=unix:///csi/csi.sock
        - --webhook-port={{ .Values.webhook.service.targetPort }}
        - --worker-threads={{ .Values.scalability.driver.workerThreads }}
        - --kube-api-qps={{ .Values.scalability.driver.kubeApi.qps }}
        - --kube-api-burst={{ .Values.scalability.driver.kubeApi.burst }}
        - --metrics-bind-address=:{{ .Values.observability.driver.metrics.port }}
        - --metrics-secure=false
        - --health-probe-bind-address=:{{ .Values.observability.driver.health.port }}
        - --trace-address={{ .Values.observability.driver.trace.endpoint }}
        - --trace-sample-rate={{ .Values.observability.driver.trace.sampleRate }}
        - --trace-service-id=$(POD_NAME)
        - --v={{ .Values.observability.driver.log.level }}
        - --enable-cleanup={{ .Values.cleanup.enabled }}
        {{- if or .Values.webhook.enforceEphemeral.enabled .Values.webhook.hyperconverged.enabled }}
        - --webhook-service-name={{ .Values.name }}-webhook-service
        {{- end }}
        {{- if .Values.webhook.enforceEphemeral.enabled }}
        - --enforce-ephemeral-webhook-config={{ .Values.name }}-enforce-ephemeral
        {{- end }}
        {{- if .Values.webhook.hyperconverged.enabled }}
        - --hyperconverged-webhook-config={{ .Values.name }}-hyperconverged-webhook
        {{- end }}
        {{- if or .Values.webhook.enforceEphemeral.enabled .Values.webhook.hyperconverged.enabled }}
        - --certificate-secret-name={{ .Values.name }}-webhook-cert
        {{- end }}
        command:
        - /local-csi-driver
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        {{- if hasPrefix "/" .Values.image.driver.repository }}
        image: "{{ .Values.image.baseRepo }}{{ .Values.image.driver.repository }}:{{ .Values.image.driver.tag | default .Chart.AppVersion }}"
        {{- else }}
        image: "{{ .Values.image.driver.repository }}:{{ .Values.image.driver.tag | default .Chart.AppVersion }}"
        {{- end }}
        imagePullPolicy: {{ .Values.image.driver.pullPolicy }}
        livenessProbe:
          httpGet:
            path: /healthz
            port: health
          initialDelaySeconds: 15
          periodSeconds: 20
        ports:
        - containerPort: {{ .Values.webhook.service.targetPort}}
          name: webhook-server
          protocol: TCP
        - containerPort: {{ .Values.observability.driver.metrics.port }}
          name: metrics
          protocol: TCP
        - containerPort: {{ .Values.observability.driver.health.port }}
          name: health
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /readyz
            port: health
          initialDelaySeconds: 5
          periodSeconds: 10
        resources: {{- toYaml .Values.resources.driver | nindent 10 }}
        securityContext:
          privileged: true
          capabilities:
            drop:
              - ALL
        terminationMessagePath: /tmp/termination-log
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
        {{- if or .Values.webhook.enforceEphemeral.enabled .Values.webhook.hyperconverged.enabled }}
        - mountPath: /tmp/k8s-webhook-server/serving-certs
          name: cert
          readOnly: true
        {{- end }}
        - mountPath: /dev
          name: device
        - mountPath: /csi
          name: csi-socket-dir
        - mountPath: /var/lib/kubelet/
          mountPropagation: Bidirectional
          name: mountpoint-dir

      - name: csi-provisioner
        args:
        - --csi-address=/csi/csi.sock
        - --node-deployment
        - --http-endpoint=:{{ .Values.observability.csiProvisioner.http.port }}
        - --retry-interval-start=1s
        - --retry-interval-max=30s
        - --worker-threads={{ .Values.scalability.csiProvisioner.workerThreads }}
        - --kube-api-qps={{ .Values.scalability.csiProvisioner.kubeApi.qps }}
        - --kube-api-burst={{ .Values.scalability.csiProvisioner.kubeApi.burst }}
        - --extra-create-metadata
        - --feature-gates=Topology=true
        - --strict-topology=true
        - --enable-capacity
        - --capacity-ownerref-level=0
        - --capacity-poll-interval=15s
        - --v={{ .Values.observability.csiProvisioner.log.level }}
        env:
        # POD_NAME and POD_NAMESPACE are used for capacity tracking support
        # More info: https://github.com/kubernetes-csi/external-provisioner?tab=readme-ov-file#capacity-support
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        {{- if hasPrefix "/" .Values.image.csiProvisioner.repository }}
        image: "{{ .Values.image.baseRepo }}{{ .Values.image.csiProvisioner.repository }}:{{ .Values.image.csiProvisioner.tag }}"
        {{- else }}
        image: "{{ .Values.image.csiProvisioner.repository }}:{{ .Values.image.csiProvisioner.tag }}"
        {{- end }}
        imagePullPolicy: {{ .Values.image.csiProvisioner.pullPolicy }}
        livenessProbe:
          failureThreshold: 1
          initialDelaySeconds: 10
          periodSeconds: 20
          tcpSocket:
            port: provisioner
          timeoutSeconds: 10
        ports:
        - containerPort: {{ .Values.observability.csiProvisioner.http.port }}
          name: provisioner
          protocol: TCP
        resources: {{- toYaml .Values.resources.csiProvisioner | nindent 10 }}
        securityContext:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
        volumeMounts:
        - mountPath: /csi
          name: csi-socket-dir

      - name: csi-resizer
        args:
        - --csi-address=/csi/csi.sock
        - --timeout=240s
        - --handle-volume-inuse-error=false
        - --http-endpoint=:{{ .Values.observability.csiResizer.http.port }}
        - --v={{ .Values.observability.csiResizer.log.level }}
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        {{- if hasPrefix "/" .Values.image.csiResizer.repository }}
        image: "{{ .Values.image.baseRepo }}{{ .Values.image.csiResizer.repository }}:{{ .Values.image.csiResizer.tag }}"
        {{- else }}
        image: "{{ .Values.image.csiResizer.repository }}:{{ .Values.image.csiResizer.tag }}"
        {{- end }}
        imagePullPolicy: {{ .Values.image.csiResizer.pullPolicy }}
        ports:
        - containerPort: {{ .Values.observability.csiResizer.http.port }}
          name: resizer
          protocol: TCP
        resources: {{- toYaml .Values.resources.csiResizer | nindent 10 }}
        securityContext:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
        volumeMounts:
        - mountPath: /csi
          name: csi-socket-dir

      - name: csi-registrar
        args:
        - --csi-address=/csi/csi.sock
        - --kubelet-registration-path=/var/lib/kubelet/plugins/localdisk.csi.acstor.io/csi.sock
        - --http-endpoint=:{{ .Values.observability.nodeDriverRegistrar.http.port }}
        - --v={{ .Values.observability.nodeDriverRegistrar.log.level }}
        env:
        {{- if hasPrefix "/" .Values.image.nodeDriverRegistrar.repository }}
        image: "{{ .Values.image.baseRepo }}{{ .Values.image.nodeDriverRegistrar.repository }}:{{ .Values.image.nodeDriverRegistrar.tag }}"
        {{- else }}
        image: "{{ .Values.image.nodeDriverRegistrar.repository }}:{{ .Values.image.nodeDriverRegistrar.tag }}"
        {{- end }}
        imagePullPolicy: {{ .Values.image.nodeDriverRegistrar.pullPolicy }}
        livenessProbe:
          failureThreshold: 1
          httpGet:
            path: /healthz
            port: registrar
          initialDelaySeconds: 5
          periodSeconds: 20
          timeoutSeconds: 5
        ports:
        - containerPort: {{ .Values.observability.nodeDriverRegistrar.http.port }}
          name: registrar
          protocol: TCP
        resources: {{- toYaml .Values.resources.nodeDriverRegistrar | nindent 10 }}
        securityContext:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
        volumeMounts:
        - mountPath: /csi
          name: csi-socket-dir
        - mountPath: /registration
          name: registration-dir

      hostPID: true
      nodeSelector: {{- toYaml .Values.daemonset.nodeSelector | nindent 8 }}
      priorityClassName: system-node-critical
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: {{ .Values.name }}-node
      terminationGracePeriodSeconds: 60
      tolerations: {{- toYaml .Values.daemonset.tolerations | nindent 8 }}
      volumes:
      {{- if or .Values.webhook.enforceEphemeral.enabled .Values.webhook.hyperconverged.enabled }}
      - name: cert
        secret:
          secretName: {{ .Values.name }}-webhook-cert
      {{- end }}
      - hostPath:
          path: /dev
          type: Directory
        name: device
      - hostPath:
          path: /var/lib/kubelet/plugins/localdisk.csi.acstor.io/
          type: DirectoryOrCreate
        name: csi-socket-dir
      - hostPath:
          path: /var/lib/kubelet/plugins_registry/
          type: Directory
        name: registration-dir
      - hostPath:
          path: /var/lib/kubelet/
          type: DirectoryOrCreate
        name: mountpoint-dir
