# Name used for creating resources.
name: csi-local

# Container image configuration.
image:
  # The base repository for the container images. Will be used when `repository`
  # begins with a '/' for a specific image.
  baseRepo: mcr.microsoft.com
  driver:
    repository: localcsidriver.azurecr.io/acstor/local-csi-driver
    # When the tag is unset (recommended), the chart version is used as the tag.
    tag:
    pullPolicy: IfNotPresent
  manager:
    repository: localcsidriver.azurecr.io/acstor/local-csi-manager
    tag:
    pullPolicy: IfNotPresent
  csiProvisioner:
    repository: /oss/v2/kubernetes-csi/csi-provisioner
    tag: v5.2.0
    pullPolicy: IfNotPresent
  csiResizer:
    repository: /oss/v2/kubernetes-csi/csi-resizer
    tag: v1.13.2
    pullPolicy: IfNotPresent
  nodeDriverRegistrar:
    repository: /oss/v2/kubernetes-csi/csi-node-driver-registrar
    tag: v2.13.0
    pullPolicy: IfNotPresent

# DaemonSet configuration.
daemonset:
  # Pod selector labels for the DaemonSet. If empty, default labels are used.
  podSelector: {}
  # Update strategy for the DaemonSet.
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 10%

  # Node selector for the DaemonSet. If empty, all nodes are selected. This can
  # be used to install the driver on a subset of nodes with local NVMe storage.
  nodeSelector: {}

  # Node affinity for the DaemonSet. If empty, no affinity rules are applied except those
  # for OS and architecture. This can be used to install the driver on a subset of nodes
  # with local NVMe storage.
  nodeAffinity: {}

  # Tolerations for the DaemonSet. If empty, no tolerations are applied.
  # This can be used to install the driver on nodes with specific taints.
  tolerations:
  - effect: NoSchedule
    operator: Exists
  - effect: NoExecute
    operator: Exists

  serviceAccount:
    # Annotations for the service account. If empty, no annotations are applied.
    annotations: {}

# RAID configuration.
# EXPERIMENTAL: This feature is experimental and may change in future releases.
# When raid.enabled is true, mdadm is used to create a RAID 0 array from unused
# NVMe devices, with LVM layered on top. When raid.enabled is false, the driver
# uses individual NVMe devices directly with LVM (no RAID striping across devices).
# Note: Migrating from mdadm-based RAID to non-RAID is not currently supported.
raid:
  # Enables mdadm-based software RAID 0 on the nodes.
  # EXPERIMENTAL: Use with caution. Once enabled, migrating away from mdadm
  # RAID is not straightforward and may require manual intervention.
  enabled: false

  # The volume group name to create on the nodes. This volume group will be used
  # for creating logical volumes for the PVCs. Must be unique on the node.
  # If the volume group already exists, it will be used as is. If this changes
  # from the default "containerstorage", then it must also be specified in the
  # StorageClass in the `VolumeGroup` parameter.
  volumeGroup: containerstorage

# Garbage collection and cleanup configuration.
cleanup:
  # Cleanup volume groups and physical volumes on pod termination if logical volume are not in use.
  enabled: true

  # LVM logical volume garbage collection configuration driven by PV annotation mismatches.
  lvGarbageCollection:
    # Enable event-driven LV garbage collection for PV node annotation mismatches
    enabled: true

  # LVM orphan cleanup configuration
  lvmOrphanCleanup:
    # Enable periodic orphaned LVM Logical Volume cleanup scanning
    enabled: true
    # Interval for scanning and cleaning up orphaned LVM volumes
    interval: 5m

  # PV cleanup controller configuration.
  # This controller watches for Released PersistentVolumes and removes finalizers
  # when nodes are unavailable, allowing PVs to be deleted.
  pvCleanup:
    # Enable the PV cleanup controller in the manager deployment
    enabled: true

# Webhook configuration.
webhook:
  enforceEphemeral:
    # Enables the enforce ephemeral PVC validation webhook, which enforces that PVCs are
    # ephemeral, or that the `localdisk.csi.acstor.io/accept-ephemeral-storage` label is set to `true`.
    enabled: true
  hyperconverged:
    # Enables the hyperconverged pod mutation webhook. Required to allow pods to
    # recover (with empty volumes) after cluster restart if node names change.
    enabled: true
  service:
    # The webhook service's port.
    port: 443
    # The target port for the webhook service. The webhook endpoint listens on
    # this port.
    targetPort: 9443
    # The type of the webhook service. Can be ClusterIP, NodePort, or
    # LoadBalancer.
    type: ClusterIP

# Manager configuration.
manager:
  serviceAccount:
    # Annotations for the service account. If empty, no annotations are applied.
    annotations: {}
  deployment:
    # Number of webhook replicas for high availability
    replicas: 2
    # Pod security context
    podSecurityContext: {}
    # Container security context
    securityContext: {}
    # Node selector for webhook pods
    nodeSelector: {}
    # Tolerations for webhook pods
    tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/controlplane"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "CriticalAddonsOnly"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "CriticalAddonsOnly"
      operator: "Exists"
      effect: "NoExecute"
    # Affinity for webhook pods (anti-affinity recommended for HA)
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: In
              values:
              - linux
            - key: kubernetes.io/arch
              operator: In
              values:
              - amd64
              - arm64
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 90
          preference:
            matchExpressions:
            - key: kubernetes.azure.com/mode
              operator: In
              values:
              - system
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - webhook
            topologyKey: kubernetes.io/hostname
    # Extra environment variables
    env: []
    # Extra arguments for the webhook
    extraArgs: []
    # Pod labels
    podLabels: {}
    # Pod annotations
    podAnnotations: {}

# Resource configuration.
resources:
  driver:
    limits:
      memory: 600Mi
    requests:
      cpu: 10m
      memory: 60Mi
  csiProvisioner:
    limits:
      memory: 100Mi
    requests:
      cpu: 10m
      memory: 20Mi
  csiResizer:
    limits:
      memory: 500Mi
    requests:
      cpu: 10m
      memory: 20Mi
  nodeDriverRegistrar:
    limits:
      memory: 100Mi
    requests:
      cpu: 10m
      memory: 20Mi
  manager:
    limits:
      memory: 128Mi
    requests:
      cpu: 10m
      memory: 64Mi

# Observability and health configuration.
observability:
  metrics:
    # Endpoints are always enabled, toggling this only affects the RBAC
    # rule creation.
    enabled: true
  manager:
    log:
      level: 2
    metrics:
      port: 8080
    health:
      port: 8081
  driver:
    log:
      level: 2
    metrics:
      port: 8080
    health:
      port: 8081
    trace:
      # The address to send traces to. Disables tracing if not set.
      endpoint: ""
      # Sample rate per million. 0 to disable tracing, 1000000 to trace everything.
      sampleRate: "1000000"
  csiProvisioner:
    log:
      level: 2
    http:
      port: 8090
  csiResizer:
    log:
      level: 2
    http:
      port: 8091
  nodeDriverRegistrar:
    log:
      level: 1
    http:
      port: 8092

# Scalability tuning.
scalability:
  driver:
    workerThreads: 100
    kubeApi:
      qps: 100
      burst: 200
  csiProvisioner:
    workerThreads: 100
    kubeApi:
      qps: 100
      burst: 200
  manager:
    kubeApi:
      qps: 100
      burst: 200
